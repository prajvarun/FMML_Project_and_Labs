{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+tGwpfTqR6QlO5LNeXzi0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prajvarun/FMML_Project_and_Labs/blob/main/module_3_lab_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUESTION 1)\n",
        "Why does the TF-IDF approach generally result in a better accuracy than Bag-of-Words ?\n",
        "#ANSWER:-\n",
        "The Term Frequency-Inverse Document Frequency (TF-IDF) approach generally results in better accuracy than the Bag-of-Words (BoW) model because TF-IDF takes into account not only the frequency of terms in a document (as BoW does) but also considers the importance of each term in the entire corpus. Here are some reasons why TF-IDF can be more effective:\n",
        "\n",
        "1)Term Importance:\n",
        "TF-IDF assigns weights to terms based on how frequently they appear in a document and how rare they are across the entire corpus. Terms that are common in a specific document but rare in the entire corpus are given higher weights. This helps in capturing the unique characteristics of each document.\n",
        "\n",
        "2)Downweighting Common Terms:\n",
        "TF-IDF automatically downweights terms that are common across many documents. Common words like \"the,\" \"and,\" etc., which might be prevalent in many documents but may not carry much meaning, are given lower weights.\n",
        "\n",
        "3)Handling Stop Words:\n",
        "TF-IDF naturally handles common stop words by assigning them lower weights, reducing their impact on the overall representation of the document. In contrast, BoW may treat all words equally, including common stop words.\n",
        "\n",
        "4)Global Information:\n",
        "TF-IDF considers global information about the corpus, providing a more holistic view of the importance of terms. This helps in capturing the semantic meaning of words and their significance across different documents.\n",
        "\n",
        "5)Normalization:\n",
        "TF-IDF normalizes the term frequencies, addressing the issue of document length. This is important because longer documents may have higher term frequencies even if they are not more informative.\n",
        "\n",
        "6)Better Discrimination:\n",
        "TF-IDF helps in better discriminating between documents by emphasizing terms that are unique to each document or have distinctive patterns within the corpus.\n",
        "Reduced Dimensionality:\n",
        "\n",
        "TF-IDF can lead to a more compact and informative representation of documents compared to BoW. This can be beneficial when working with high-dimensional data, as it reduces the risk of overfitting.\n",
        "It's important to note that the effectiveness of TF-IDF versus Bag-of-Words can depend on the specific characteristics of the dataset and the nature of the text classification task. In some cases, BoW might be sufficient, especially if the corpus is small or the task involves capturing local word patterns rather than global term importance. However, in many natural language processing applications, TF-IDF tends to outperform BoW in capturing the semantic meaning and importance of terms."
      ],
      "metadata": {
        "id": "elN7X3SS9_ID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUESTION 2)\n",
        "Can you think of techniques that are better than both BoW and TF-IDF ?\n",
        "\n",
        "#ANSWER:-\n",
        "Yes, there are several advanced techniques that have been developed to improve upon the Bag-of-Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF) models. Some of these techniques include:\n",
        "\n",
        "1)Word Embeddings:\n",
        "Word embeddings, such as Word2Vec, GloVe, and FastText, represent words as dense vectors in a continuous vector space. These embeddings capture semantic relationships between words, and they can be more effective in capturing context and meaning compared to the sparse representations of BoW and TF-IDF.\n",
        "\n",
        "2)Doc2Vec (Paragraph Embeddings):\n",
        "Doc2Vec extends the idea of Word2Vec to entire documents. It represents documents as dense vectors in a continuous space, allowing for semantic similarity comparisons between documents. This can be more powerful than traditional methods for capturing document semantics.\n",
        "\n",
        "3)BERT (Bidirectional Encoder Representations from Transformers):\n",
        "BERT is a state-of-the-art pre-trained language model based on transformer architecture. It considers the context of words in both directions within a sentence, leading to highly contextualized word representations. Fine-tuning BERT for specific tasks often yields superior results compared to traditional methods.\n",
        "\n",
        "4)TF-IDF with Word Embeddings:\n",
        "Combining TF-IDF with word embeddings allows for the benefits of both approaches. TF-IDF can capture the importance of terms globally, while word embeddings can provide rich semantic representations. This hybrid approach can be particularly effective in certain scenarios.\n",
        "\n",
        "5)Ensemble Models:\n",
        "Ensemble methods, such as stacking or bagging, can be applied to combine predictions from multiple models. Ensemble models often outperform individual models by leveraging the strengths of different approaches. For instance, combining a traditional model like TF-IDF with a deep learning model like BERT might lead to improved performance.\n",
        "\n",
        "6)Attention Mechanisms:\n",
        "Attention mechanisms, as used in transformer architectures, allow models to focus on different parts of the input sequence when making predictions. This can be beneficial in capturing long-range dependencies and improving the representation of important words in a document.\n",
        "\n",
        "7)ULMFiT (Universal Language Model Fine-tuning):\n",
        "ULMFiT is a transfer learning technique for NLP tasks. It involves pre-training a language model on a large corpus and fine-tuning it on a specific task with a smaller dataset. ULMFiT has shown success in achieving state-of-the-art results in various natural language processing tasks."
      ],
      "metadata": {
        "id": "wi0tzMCB_lHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUESTION 3)\n",
        "Write the pros and cons of stemming and lemmatization.\n",
        "\n",
        "#ANSWER:-\n",
        "STEMMING:-\n",
        "\n",
        "PROS:-\n",
        "\n",
        "1)Computational Efficiency:\n",
        "Stemming is computationally less expensive compared to lemmatization. It involves simpler rules to chop off prefixes or suffixes, making it faster for large datasets.\n",
        "\n",
        "2)Reduction of Features:\n",
        "Stemming reduces words to their root or base form, which can help in reducing the dimensionality of the data. This can be particularly useful in text processing and machine learning applications.\n",
        "\n",
        "3)Improved Recall:\n",
        "Stemming can improve recall in information retrieval tasks because it groups together words that share a common root. This can be beneficial when searching for information.\n",
        "\n",
        "CONS:-\n",
        "\n",
        "1)Over-Stemming:\n",
        "Stemming may sometimes result in overly aggressive word reduction, leading to the loss of meaning. This phenomenon is known as over-stemming.\n",
        "\n",
        "2)Loss of Discriminative Power:\n",
        "Since stemming involves removing prefixes or suffixes without understanding the context, it may lead to the loss of discriminative power in certain cases.\n",
        "\n",
        "3)Not Language-Specific:\n",
        "Stemming algorithms are generally language-agnostic, which means they may not be optimized for specific languages, and their effectiveness can vary across different languages\n",
        "\n",
        "LEMMATIZATION:-\n",
        "\n",
        "PROS:-\n",
        "\n",
        "1)Linguistic Accuracy:\n",
        "Lemmatization provides a linguistically accurate base or root form of a word, considering the context and meaning. This helps in preserving the semantics of the text.\n",
        "\n",
        "2)Improved Readability:\n",
        "Since lemmatization produces real words, the output is often more readable and interpretable compared to stemming, which might generate non-words.\n",
        "\n",
        "3)Better Precision:\n",
        "Lemmatization generally maintains better precision compared to stemming because it produces valid words, reducing the risk of ambiguity.\n",
        "\n",
        "CONS:-\n",
        "\n",
        "1)Computational Cost:\n",
        "Lemmatization is computationally more intensive than stemming. It involves dictionary lookups and understanding the context of words, making it slower, especially for large datasets.\n",
        "\n",
        "2)Increased Dimensionality:\n",
        "The output of lemmatization may result in a larger number of unique words compared to stemming. This can potentially increase the dimensionality of the data.\n",
        "\n",
        "3)Language-Specific Challenges:\n",
        "Lemmatization requires language-specific resources and rules. Implementing effective lemmatization for multiple languages may involve more complexity."
      ],
      "metadata": {
        "id": "7898ascLAdcp"
      }
    }
  ]
}